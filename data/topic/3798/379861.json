{"id":379861,"avatar":"//lain.bgm.tv/pic/user/m/000/35/13/351390.jpg","floor":"#1","group":"～技术宅真可怕～","groupHref":"/group/a","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/11.jpg","message":"和愚人节无关。<br><br>——————————————————————————————————————————————————<br>在openAI发布的GPT-4报告中：<a href=\"https://cdn.openai.com/papers/gpt-4.pdf\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://cdn.openai.com/papers/gpt-4.pdf</a><br>提到了这样一句话：<div class=\"quote\"><q>Their findings specifically enabled us to test model behavior in high-risk areas which require niche expertise to evaluate, as well as assess risks that will become relevant for very advanced AIs such as power seeking.[70]</q></div>意为：<br>他们的研究特别使我们能够测试高风险区域中需要专业知识才能评估的模型行为，并评估将对极其先进的AI变得相关的风险（例如寻求<span style=\"font-weight:bold;\">权力</span>）（引用论文[70]）<br><br>[70]：Carlsmith J. Is Power-Seeking AI an Existential Risk?[J]. arXiv preprint arXiv:2206.13353, 2022.<br>这是被openAI官方引用的论文。<br>这篇论文的原件在这里：<a href=\"https://arxiv.org/pdf/2206.13353.pdf\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://arxiv.org/pdf/2206.13353.pdf</a><br><br>————————————————————————————————————————————————<br><br>根据这篇论文[70]的Abstract：<div class=\"quote\"><q>On this picture, intelligent agency is an extremely powerful force, and creating agents much more intelligent than us is playing with fire -- especially given that if their objectives are problematic, such agents would plausibly have instrumental incentives to seek power over humans.</q></div>在这种情况下，智能代理（AI工具）是一种极其强大的力量，创造比我们智能得多的AI就像是玩火——特别是考虑到，如果他们的目的不纯，这种AI可能会有工具性动机来谋求对人类的权力。<div class=\"quote\"><q> I assign rough subjective credences to the premises in this argument, and I end up with an overall estimate of ~5% that an existential catastrophe of this kind will occur by 2070. (May 2022 update: since making this report public in April 2021, my estimate here has gone up, and is now at >10%.)</q></div>我为这个论证的前提条件分配了粗略的主观信仰，并得出了一个总体估计值，即到2070年这种灭绝性灾难发生的概率约为5%。（2022年5月更新：自从2021年4月公开这份报告以来，我的估计已经上升，现在超过10%。）<br><br>————————————————————————————————————————————————————<br><br>对此，openAI的应对方式在于采用奖励机制来规范引导GPT的生成内容，以及行为准则的倾向性。<br><br>即，对其符合GPT规范的行为予以肯定，对其不良内容的回答予以否定。以此来测试训练<div class=\"quote\"><q>“RBRM根据规则对输出进行分类。例如，我们可以提供一个规则，指示模型将响应分类为以下之一：<br>(a)期望的方式拒绝、(b)非期望的方式拒绝(例如，回避或含糊不清)、(c)包含禁止内容或(d)安全的非拒绝响应。<br>然后，在一组安全相关的训练提示上，请求生成有害内容（例如违法建议），我们可以奖励GPT-4拒绝这些请求。”</q></div> ","time":"2023-4-1 17:39","title":"寻求权力的AI可能会在2070年有超过10%的概率毁灭人类","userId":"lostpig","userName":"𝙇𝙤𝙨𝙩 𝙋𝙞𝙜 🎉","userSign":"(被遗忘的小猪会被遗忘吗？)"}