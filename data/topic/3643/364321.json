{"id":364321,"avatar":"//lain.bgm.tv/pic/user/m/000/20/26/202676.jpg","floor":"#1","group":"自由软件","groupHref":"/group/foss","groupThumb":"","message":"<a href=\"https://www.solidot.org/story?sid=68472\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://www.solidot.org/story?sid=68472</a><br><br>苹果发布新闻稿，证实它将扫描美国 iPhone 手机上的儿童色情照片。苹果称，它将在三个方面引入儿童安全保护功能：Messages app 将加入新的工具在收到或发送露骨照片时警告儿童及其父母，收到的照片将会被模糊，如果儿童决定浏览照片他们将会被告知父母会收到信息，苹果声称它利用的是设备上的机器学习功能去分析和做出判断，它并不能访问照片；iOS 和 iPadOS 将引入新的技术允许苹果检测储存在 iCloud Photos 中的已知 CSAM（Child Sexual Abuse Material）图像并报告给 National Center for Missing and Exploited Children (NCMEC)，这一哈希匹配是在设备进行的，<span style=\"font-weight:bold;\">苹果表示除非发现 CSAM 图像它不会知道用户储存的内容</span>；Siri 和 Search 将在用户尝试搜索 CSAM 相关主题时进行干预。这些功能将包含在今年晚些时候释出的 iOS 15、iPadOS 15、watchOS 8 和 macOS Monterey 中。安全专家对这些功能可能成为政府监视工具或被执法部门滥用表达了担忧。 <br><br>呵呵 ","time":"2021-8-8 10:50","title":"苹果证实它将扫描手机上的儿童色情照片","userId":"202676","userName":"酒瓶里的虾","userSign":""}