{"id":379564,"avatar":"//lain.bgm.tv/pic/user/m/000/42/12/421268.jpg","floor":"#1","group":"站务论坛","groupHref":"/group/forum","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/2.jpg","message":"点一个人就能把和他共事过的人际图输出<br><br>让GPT帮我写了个程序，但是会出错。凭借着我自学那一点python属实不知道该怎么搞了。<br><br>【这个错误日志表明，你的程序遇到了一个 SSLError，它表示无法通过 HTTPS 连接到 bangumi.tv 网站，具体原因为 EOF（文件尾）违反协议。<br><br>这种错误可能由多种原因引起，其中一些可能包括：<br><br>网络连接问题。请确保你的计算机可以正常连接到互联网，并且没有被防火墙或其他安全软件阻止。<br><br>你提供的 URL 不正确。确保你使用的是正确的 URL，而不是示例代码中提供的 <a href=\"https://bgm.tv/person/21580\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://bgm.tv/person/21580</a>。<br><br>服务器问题。如果 bangumi.tv 服务器遇到了问题，则可能导致此错误。<br><br>Python 环境问题。尝试更新你的 Python 环境或使用其他 Python 环境（如 Anaconda）。<br><br>SSL/TLS 证书问题。可能存在证书问题，例如证书已过期或不匹配。你可以尝试使用 requests 库提供的 verify 参数来禁用证书验证，例如 verify=False。】<br><br><span style=\"font-weight:bold;\">这里把代码奉上：</span><br>import requests<br>from bs4 import BeautifulSoup<br><br>def fetch_staff_data(staff_member_url):<br>    response = requests.get(staff_member_url)<br>    soup = BeautifulSoup(response.text, 'html.parser')<br>    return soup<br><br>def extract_works_and_co_workers(soup):<br>    works_and_co_workers = {}<br>    <br>    # Locate the elements containing the works and co-workers data.<br>    # The CSS selectors used here may vary depending on the website's structure.<br>    works_elements = soup.select('.work-list-element')<br>    co_workers_elements = soup.select('.co-workers-list-element')<br><br>    for work, co_workers in zip(works_elements, co_workers_elements):<br>        work_title = work.get_text()<br>        co_worker_names = [co_worker.get_text() for co_worker in co_workers.select('.co-worker-name')]<br>        works_and_co_workers[work_title] = co_worker_names<br><br>    return works_and_co_workers<br><br>def main():<br>    staff_member_url = '<a href=\"https://bgm.tv/person/21580\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://bgm.tv/person/21580</a>'  # Replace with the actual URL of the staff member you're interested in.<br>    soup = fetch_staff_data(staff_member_url)<br>    works_and_co_workers = extract_works_and_co_workers(soup)<br><br>    # Process the data and output the results as desired.<br>    for work, co_workers in works_and_co_workers.items():<br>        print(f\"{work}:\")<br>        for co_worker in co_workers:<br>            print(f\"  - {co_worker}\")<br><br>if __name__ == '__main__':<br>    main() ","time":"2023-3-26 10:52","title":"bangumi能不能搞一个关联系统啊","userId":"421268","userName":"赤坂羽","userSign":"(新时代的破船爱谁登谁登)"}