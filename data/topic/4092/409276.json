{"id":409276,"avatar":"//lain.bgm.tv/pic/user/m/000/37/04/370405.jpg","floor":"#1","group":"番组开发","groupHref":"/group/dev","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/41/4180.jpg?r=1519373244","message":"<span style=\"font-weight:bold;\">预览</span><br><img width=\"639\" height=\"300\" src=\"https://p.sda1.dev/20/345e65ed75193a815bad9c5dddd8be76/%E5%9B%BE%E7%89%87.png\" border=\"0\" alt class=\"code\" referrerpolicy=\"no-referrer\"><br><br><span style=\"font-weight:bold;\">功能</span><br>- 使用 <a href=\"https://highlightjs.org/\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">highlight.js</a> 自动检测代码块语言并高亮<br>- 添加悬浮显示的一键复制按钮<br>- 支持夜间模式<br>- 兼容性<br>  - 兼容<a href=\"https://github.com/inchei/userstyles\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">bangumi 僞質感設計样式</a><br>  - 大概率不兼容<a href=\"https://bgm.tv/dev/app/1049\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">代码块超进化！</a><br><br><span style=\"font-weight:bold;\">链接</span><br><a href=\"https://bgm.tv/dev/app/3377\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">组件</a><br><a href=\"https://greasyfork.org/zh-CN/scripts/516547-%E7%8F%AD%E5%9B%BA%E7%B1%B3%E4%BB%A3%E7%A0%81%E9%AB%98%E4%BA%AE\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">Greasy Fork</a><br><br><span style=\"font-weight:bold;\">更新日志</span><br><div class=\"codeHighlight\"><pre>1.3 修复行高；设置 plaintext<br>1.2 限制最大高度<br>1.1 删去难绷的识别语言显示，修改为悬浮显示复制按钮</pre></div><br><br><span style=\"font-weight:bold;\">示例</span><br><div class=\"codeHighlight\"><pre>import numpy as np<br>import torch<br><br>def solve_block_tridiagonal(a, b, c, d):<br><br>    N = len(b)<br>    x = np.zeros_like(d)<br>    <br>    # Forward elimination with explicit C* and d* storage<br>    C_star = np.zeros_like(c)<br>    d_star = np.zeros_like(d)<br><br>    # Initial calculations for C_0* and d_0*<br>    C_star[0] = np.linalg.solve(b[0], c[0])<br>    d_star[0] = np.linalg.solve(b[0], d[0])<br><br>    # Forward elimination<br>    for i in range(1, N - 1):<br>        C_star[i] = np.linalg.solve(b[i] - a[i-1] @ C_star[i-1], c[i])<br>        d_star[i] = np.linalg.solve(b[i] - a[i-1] @ C_star[i-1], d[i] - a[i-1] @ d_star[i-1])<br><br>    # Last d_star update for the last block<br>    d_star[-1] = np.linalg.solve(b[-1] - a[-2] @ C_star[-2], d[-1] - a[-2] @ d_star[-2])<br><br>    # Backward substitution<br>    x[-1] = d_star[-1]<br>    for i in range(N-2, -1, -1):<br>        x[i] = d_star[i] - C_star[i] @ x[i+1]<br><br>    return x<br><br><br>def test_block_tridiagonal_solver():<br><br>    N = 4<br><br>    a = np.array([<br>        [[1, 0.5], [0.5, 1]],  <br>        [[1, 0.5], [0.5, 1]],<br>        [[1, 0.5], [0.5, 1]]<br>    ], dtype=np.float64)<br>    <br>    b = np.array([<br>        [[5, 0.5], [0.5, 5]],  <br>        [[5, 0.5], [0.5, 5]],<br>        [[5, 0.5], [0.5, 5]],<br>        [[5, 0.5], [0.5, 5]]<br>    ], dtype=np.float64)<br>    <br>    c = np.array([<br>        [[1, 0.5], [0.5, 1]],  <br>        [[1, 0.5], [0.5, 1]],<br>        [[1, 0.5], [0.5, 1]]<br>    ], dtype=np.float64)<br><br>    d = np.array([<br>        [1, 2], <br>        [2, 3], <br>        [3, 4], <br>        [4, 5]<br>    ], dtype=np.float64)<br>    <br>    x = solve_block_tridiagonal(a, b, c, d)<br><br>    # Construct the equivalent full matrix A_full and right-hand side d_full<br>    A_full = np.block([<br>        [b[0], c[0], np.zeros((2, 2)), np.zeros((2, 2))],<br>        [a[0], b[1], c[1], np.zeros((2, 2))],<br>        [np.zeros((2, 2)), a[1], b[2], c[2]],<br>        [np.zeros((2, 2)), np.zeros((2, 2)), a[2], b[3]]<br>    ])<br>    <br>    d_full = d.flatten()  # Flatten d for compatibility with the full system<br><br>    # Solve using numpy's direct solve for comparison<br>    x_np = np.linalg.solve(A_full, d_full).reshape((N, 2))<br>    # Print the solutions for comparison<br>    print(\"Solution x from block tridiagonal solver (TMDA):\\n\", x, \"\\nResidual:\", torch.sum(torch.abs(torch.tensor(A_full)@torch.tensor(x).flatten() - torch.tensor(d).flatten())))<br>    print(\"Solution x from direct full matrix solver:\\n\", x_np, \"\\nResidual np:\", torch.sum(torch.abs(torch.tensor(A_full)@torch.tensor(x_np).flatten() - torch.tensor(d).flatten())))<br># Run the test function<br>test_block_tridiagonal_solver()</pre></div>","time":"2024-11-9 21:24","title":"[组件/脚本] 代码高亮 / 一键复制","userId":"inchei","userName":"茵陳","userSign":"(Hello darkness my old friend)"}