{"id":393437,"avatar":"//lain.bgm.tv/pic/user/m/000/84/78/847831.jpg","floor":"#1","group":"～技术宅真可怕～","groupHref":"/group/a","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/11.jpg","message":"上期回顾：<a href=\"https://chii.in/group/topic/393208\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">开坑！用夜羊社洛丽塔系列的脚本训练语言大模型：数据集开源</a><br><br>6B 的模型语言能力还比较弱，驾驭不了剧情<span style=\"text-decoration: line-through;\">，不过好在夜羊社作品里也没太多剧情</span>。<br><br>另外，按理说，夜羊社的脚本包含诗和睡觉，但是对于 6B 来说诗还是有点难，所以目前嘛，只擅长睡觉（逃<br><br><span style=\"font-size:20px; line-height:20px;\">好，上图</span><br><br>（图片用 yuzutalk 制作的，用爱丽丝的头像只是因为她是机娘，邦邦咔邦！<br><br>“日常”<br><br><img src=\"https://p.sda1.dev/15/19b1704218d294eee57b2e1c5cd6ddf7/sol_sol.png\" class=\"code\" rel=\"noreferrer\" referrerpolicy=\"no-referrer\" alt><br><br>“非常的日常”<span style=\"text-decoration: line-through;\">（因为赶时间所以是速通版</span><br><br><img src=\"https://p.sda1.dev/15/d2acbc2a06a457d45087c24c2421436e/uol_fast_f16.png\" class=\"code\" rel=\"noreferrer\" referrerpolicy=\"no-referrer\" alt><br><br>虽然没有特别针对叙述描写训练，但算是能写一点吧<br><br><img src=\"https://p.sda1.dev/15/90c23a751c8e110a6c08d83aefa5d1d2/narrate_haiku_scarf.png\" class=\"code\" rel=\"noreferrer\" referrerpolicy=\"no-referrer\" alt><br><img src=\"https://p.sda1.dev/15/e513987e3df4f59f7a8487150cb51ca2/narrate_haiku_abyss.png\" class=\"code\" rel=\"noreferrer\" referrerpolicy=\"no-referrer\" alt><br><br><br><span style=\"font-size:20px; line-height:20px;\">你先别急，使用技巧我只讲这一个</span><br><br>对于一些比较难开头的对话，可以为模型手动填充一些对话，就是修改对方的对话为自己期待的回应。比如在上面例子中标有“（手动填充）”的那些内容都是我给加上去的。把大型语言模型想象成一个学东西很快的小孩子，不要用说教的方式，而是要身体力行地示范。<br>另外，毕竟训练内容都是夜羊社的脚本，所以只要你的说话方式够夜羊，就能得到较好的回应。<br>还有一件事，重复多生成几次，总能抽到大保底。<br><br><br><span style=\"font-size:20px; line-height:20px;\">快端上来罢</span><br><br><span style=\"font-weight:bold;\">注意：GGUF 模型（相当于一种有损压缩）对生成质量有可见的负面影响，具体效果对比见我会发在回帖里</span><br><br>请问你是？<br><br>- （全平台，推荐）……你有没有那种，呃，Google Colab，我想白嫖 GPU……<br>  - 有！→ vLLM Colab<br>- 我是 Windows 用户，我有N卡或A卡 / 我是 macOS M系列芯片用户<br>  - → LM-Studio (GGUF 模型)<br>- 我用 iPhone/iPad，内存有 6GB 或以上<br>  - → LLMFarm (GGUF 模型)<br>- 统统闪开，我知道自己要做什么<br>  - → 为勇士准备的生肉<br><br><br><br><span style=\"font-size:18px; line-height:18px;\">vLLM Colab</span><br><br>在 Google Colab 里加载模型作为服务端，然后用本地的客户端连接<br><br><span style=\"font-weight:bold;\">服务端</span><br><br>戳这个 》》》<a href=\"https://colab.research.google.com/drive/1BO2hLaE4Ev91-UsHvjeIl-uM5FFY9qYL?usp=sharing\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://colab.research.google.co ... FFY9qYL?usp=sharing</a><br><br>执行安装代码，再执行运行代码。在看见像<div class=\"codeHighlight\"><pre>https://|xxxxxx|.trycloudflare.com</pre></div>这样的输出的时候就代表初始化完成，开始运行了。<br><br>等待的时候，去下载客户端吧。<br><br><span style=\"font-weight:bold;\">客户端</span><br><br>Chatbox，从其官网下载：<a href=\"https://chatboxai.app\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://chatboxai.app</a> （或者你有自己用着顺手的 OpenAI 客户端都可以，设置方法大同小异）<br><br>设置：<br><br>- AI 模型提供方：选 OpenAI API<br>- OpenAI API 密钥：留空不填<br>- API 域名：填你在服务端看到的那个 <div class=\"codeHighlight\"><pre>https://|xxxxxx|.trycloudflare.com</pre></div><br>- 模型&Token<br>  - 模型：选 自定义模型<br>  - 自定义模型名：填 nenekochan/Yi-6B-yoruno<br><br><span style=\"font-weight:bold;\">保存！</span><br><br>在聊天界面，把那个默认的 system prompt（齿轮图标的那条信息）删了。然后，嘿嘿，可以了哟。<br><br>Colab 有每天的 GPU 用量限制，不用的时候，记得先停止运行（点一下那个转动的播放图标）并右上角菜单内断开连接并删除运行时，再关标签页。<br><br><br><span style=\"font-size:18px; line-height:18px;\">LM Studio</span><br><br>软件，从其官网下载：<a href=\"https://lmstudio.ai\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://lmstudio.ai</a><br><br>网上能找到些教程，我这里就简单讲讲。<br><br><span style=\"font-weight:bold;\">模型下载</span><br><br>如果你能前往城墙之外，可以在软件内下载模型，点左边栏&#x1F50D;图标，搜“Yi-6B-yoruno-GGUF”。<br><br>按照自己设备的显存大小，选择一个最大的下载：<br><br>- .q8_0.gguf (≥8G显存)<br>- .q5_k_m.gguf (≥6G显存)<br>  - OneDrive 源：<a href=\"https://1drv.ms/u/s!AmDfq64kq3L9e5tLZm-PzRhfMcE?e=vRfcJE\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://1drv.ms/u/s!AmDfq64kq3L9e5tLZm-PzRhfMcE?e=vRfcJE</a><br><br>如果你无法前往城墙之外，可以手动从我的OneDrive源下载<br><a href=\"https://1drv.ms/u/s!AmDfq64kq3L9e5tLZm-PzRhfMcE?e=vRfcJE\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://1drv.ms/u/s!AmDfq64kq3L9e5tLZm-PzRhfMcE?e=vRfcJE</a><br>软件内点左边栏&#x1F4C1;图标，点“Reveal in ...”按钮，在这个存模型的目录里，新建“nenekochan”文件夹，再在里面新建“Yi-6B-yoruno-GGUF”文件夹，把下好的 GGUF 文件放在最内层。<br><br><span style=\"font-weight:bold;\">开始对话</span><br><br>点左边栏&#x1F4AC;图标，然后右边栏顶端 Preset 下拉选项选 Import Preset From file...，加载我的这个 JSON 配置文件：<br><a href=\"https://1drv.ms/u/s!AmDfq64kq3L9fOnKvB5BkoAf0xU?e=0KomPy\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://1drv.ms/u/s!AmDfq64kq3L9fOnKvB5BkoAf0xU?e=0KomPy</a><br><br>顶端横幅下拉选项，选择模型来加载。其余的应该比较直观了。<br><br>就酱。<br><br>LM Studio 还是相对好用，bug 比较少，而且可以编辑对话历史。<br><br><br><span style=\"font-size:18px; line-height:18px;\">LLMFarm</span><br><br>这个方案也就凑合能用，推荐先看看上面的 vLLM Colab<br><br>App：<a href=\"https://apps.apple.com/ru/app/llm-farm/id6461209867\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://apps.apple.com/ru/app/llm-farm/id6461209867</a><br><br>模型按照自己设备的显存大小，选择一个最大的下载：<br><a href=\"https://huggingface.co/nenekochan/Yi-6B-yoruno-GGUF/tree/main\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://huggingface.co/nenekochan/Yi-6B-yoruno-GGUF/tree/main</a><br>- .q8_0.gguf (≥8G显存)<br>- .q5_k_m.gguf (≥6G显存)<br>  - OneDrive 源：<a href=\"https://1drv.ms/u/s!AmDfq64kq3L9e5tLZm-PzRhfMcE?e=vRfcJE\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://1drv.ms/u/s!AmDfq64kq3L9e5tLZm-PzRhfMcE?e=vRfcJE</a><br><br>设置：<br><br>- Prompt format<br>  - Format：<div class=\"codeHighlight\"><pre>{{prompt}}</s></pre></div><br>  - Reverse prompt：<div class=\"codeHighlight\"><pre></s></pre></div><br>- Prediction options：这几个开关都是加速的，机器要是支持的话可以打开<br><br>保存！<br><br>因为不能编辑对话，得用奇技淫巧做手动填充，比如说，对话可以这么开始：<br><br><div class=\"codeHighlight\"><pre><br><s>我摸了摸茉子的头。<br>我：“嘿嘿，想……”</s>茉子：“想什么？”</s>我：“茉子很可爱，想跟茉子……”  <br></pre></div><br><br><br><br><span style=\"font-size:18px; line-height:18px;\">为勇士准备的生肉（误</span><br><br>- 嗯，这里是融合后的完整 f16 模型：<a href=\"https://huggingface.co/nenekochan/Yi-6B-yoruno\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://huggingface.co/nenekochan/Yi-6B-yoruno</a><br>  这是“无损”的模型，能复现我贴出来的对话实例。<br>  Linux/Windows WSL + N卡/A卡 推理推荐使用 vLLM（我在上面 Colab 中用的就是 vLLM），速度极快且可转 OpenAI API<br>- 这里是融合前的 PEFT LoRA：<a href=\"https://huggingface.co/nenekochan/Yi-6B-yoruno-peft\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://huggingface.co/nenekochan/Yi-6B-yoruno-peft</a><br>- 如果对训练细节感兴趣，可以看看<a href=\"https://huggingface.co/nenekochan/Yi-6B-yoruno\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">README</a><br><br><br><br><span style=\"font-size:20px; line-height:20px;\">（兴奋地搓手手</span><br><br>6B 的模型终究上限很低，而我这业余的微调训练大家就看个乐吧。模型总是在发展更替，唯数据集长存。我总觉得，大型语言模型这个领域只要躺着等几个月就会生长出新的惊喜，所以接下来就是继续的等待。<br><br>最后，希望有好心人帮忙转发贴吧和Q群（我都没有账号）、帮忙转存模型和数据（我没有魔搭账号），注明来源即可。<br>最后的最后，还是希望在不久的将来见到有人用这个数据集训练出更好的模型。","time":"2024-2-8 15:51","title":"用夜羊社洛丽塔系列的脚本训练语言大模型：6B对话模型开放下载","userId":"nenekochan","userName":"neneko","userSign":"(_____的里界名义)"}