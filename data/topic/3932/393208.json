{"id":393208,"avatar":"//lain.bgm.tv/pic/user/m/000/84/78/847831.jpg","floor":"#1","group":"～技术宅真可怕～","groupHref":"/group/a","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/11.jpg","message":"我提取了脚本，清理，然后把文本切分成了可用于训练的对话形式。<br><br>整理好的训练数据<span style=\"text-decoration: line-through;\">（犯罪证据）</span>放到 Hugging Face 上了。<br><br><a href=\"https://huggingface.co/datasets/nenekochan/yoruno-vn\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">》》》这里《《《</a>（Hugging Face 位于城墙之外）<br><br>当然，我去掉了所有西瓜肚的内容（抱歉，唯独这个，我…<br><br><br><span style=\"font-size:18px; line-height:18px;\">约定</span><br><br>- <span style=\"font-weight:bold;\">一星期后</span>会发布用数据中对话为主的内容做微调训练的一个 7b 左右的模型，以能在手机上运行为目标，希望做一个让大家可以简单愉快玩耍的对话模型。<br>- 未来会发布用全量数据集做微调训练的一个 30~50b 的写作模型。模型能在对话之余，比较好地驾驭旁白描写刻画叙述，就是半自动写脚本<span style=\"text-decoration: line-through;\">（把 porori 囚禁在显卡里）</span>。<br>- 只要我心中寂寞的深度没变，数据集和模型都会持续更新。<br><br><br><br><span style=\"font-size:18px; line-height:18px;\">碎碎念</span><br><br>首先，感谢数据源背后的汉化组们，他们才是真正的英雄。<br><br>离“<a href=\"https://twitter.com/FanKetchup/status/1736402131144618255\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">大黄油</a>”又近了一步 <br><br>隔壁家叔叔圣诞节了还在给盖噜给脚本做标注，你可千万不能变成那样.jpg<br><br>怎么说呢，做标注的感觉就是，用 vscode 推 galgame，或许有点能体会到汉化组的感觉 XD<br><br>有时候标注到一些非常夜羊味的文字，心里总是担心着，这样细腻的笔触也是能被模拟的吗？<span style=\"text-decoration: line-through;\">心意有好好传达给大型语言模型吗？</span><br><br>最后，希望有好心人帮忙转发贴吧和Q群（我都没有账号）、帮忙转存数据（我没有魔搭账号），注明来源即可。<br>最后的最后，希望在不久的将来见到有人用这个数据集训练出更好的模型。","time":"2024-2-2 09:00","title":"开坑！用夜羊社洛丽塔系列的脚本训练语言大模型：数据集开源","userId":"nenekochan","userName":"neneko","userSign":"(_____的里界名义)"}