{"id":415041,"avatar":"//lain.bgm.tv/pic/user/m/000/56/90/569015.jpg","floor":"#1","group":"～技术宅真可怕～","groupHref":"/group/a","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/11.jpg","message":"rt，纯CPU运行，能在低端安卓设备使用就更好了，主要是想通过脚本调用本地api的形式实现这个帖子的思路<br><a href=\"https://bgm.tv/group/topic/414607\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">基于LLM的文字混淆加密器</a><br><br>纯小白，测试过win下的ollama，跑qwen2.5:0.5B，模型才400M，内存占800M，但安装ollama就需要4个G，实在不利于推广，所以想问下有无更加轻量的针对cpu优化的部署方式","time":"2025-1-26 09:23","title":"本地部署llm最轻量的方式是？","userId":"arthur_0","userName":"降龙十八掌","userSign":"(混沌邪恶中。。。)"}