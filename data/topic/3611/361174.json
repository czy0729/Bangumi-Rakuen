{"id":361174,"avatar":"//lain.bgm.tv/pic/user/m/000/10/62/106205.jpg","floor":"#1","group":"宽带综合症候群","groupHref":"/group/download","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/35.jpg","message":"我的家庭宽带网速较慢，上传1TiB需要至少4天时间，下载1TiB需要约1天。哪天要是有哪家网盘公司允许我把硬盘和电脑带到数据中心直接拷贝那样就好啦。如果机械硬盘的发展是容量增长，读取速度却增长不大的话，你还得在数据中心住一晚上才能完成初次备份。<br>目前我在使用的大容量存储服务有 OneDrive 和 Google 云端硬盘。它们能提供较长时间的免费的 100TiB 以上的存储服务。<br>OneDrive 存储计划是E5开发者90天25x5TiB（可自助续期90天和扩容至25x25TiB），中国大陆直连亚洲服务器速度还行，高峰时段有时会慢。<br>Google 云端硬盘用的是无限容量的团队共享硬盘，不明期限，中国大陆无法直连。Google 强大的地方在于它提供的 Colab 云主机允许你<span style=\"text-decoration: line-through;\">直接操作</span>云盘中的数据（并不，修改后的文件需要一段时间完整上传写回云盘，<span style=\"font-weight:bold;\">不支持文件分块差异同步</span>，大文件就算只改了1位同步也很慢），I/O<span style=\"text-decoration: line-through;\">很快</span>(并不，数据缓存后速度才快，缓存前读取只有差不多3百万二进制字节每秒，应该是真实的网速）。缺点是网速稍慢，免费版单任务最多只能执行12小时，这样一次大概能从 OneDrive 同步约 500GiB 数据。<br><br>使用备份工具分块加密上传数据到 OneDrive 之后，还有数据校验问题。<br>OneDrive Business 可以提供文件的 QuickXOR 校验值，Google Drive 可以提供文件的md5 校验值。不巧的是我使用的备份工具使用其它种类的校验值，因此校验数据必须要读取一次，这项工作最好是在网速快的云主机或 Colab 完成。读取数据，删除出错的分块，然后同步回 OneDrive，之后重新上传损坏出错的那部分，同步到Google云盘并再次检查新增数据的完整性。<br>关于 OneDrive Business 的问题：<br>1. 目前用 Rclone 上传 OneDrive 大概每上传1万个约5百万字节大小的文件就有1个文件损坏；上传时时常报错“500服务不可用”。<br>2. 单个文件夹内对象数量超过 5000 个则不可执行重命名、移动、删除或共享、编辑权限等操作，所以在放置大量备份仓库文件前就要预先设置好。<br>3. 文件搜索引擎有时会搜索到已删除的文件。<br>4. <span style=\"font-weight:bold;\">单个列表或库*中最多存储3000万个项目或文件，单文件最大100GiB</span>。（*SharePoint 中的网站就是一种库，OneDrive 是一种网站）<br><br>关于 Google Drive 和 Colab 的问题：<br>1. 如果在 Colab 使用 Rclone 同步文件时达到团队共享文件夹上传限制，则同步的文件会自动移动到你的个人云盘根目录中……（手动删除这几千个路径不对的文件真的超麻烦，为什么不直接报错呢？）<br>2. Colab 列出 Google Drive 目录有时有Bug，延迟很大。明明 Google Drive 网页版中有很多文件，却在 Colab 中看不到……用 ls 命令查看过个几秒钟才多蹦出来几个。网页版也是一样，Colab 中修改的文件要过一段时间才能在网页版看到。这是因为挂载的云盘文件系统<span style=\"font-weight:bold;\">不支持随机写入云盘文件</span>，而是先写入主机约70G的临时空间中，然后上传到云盘……速度大概33秒每GiB。<br>3. <span style=\"font-weight:bold;\">一个共享云端硬盘最多存储 40 万个文件或文件夹，单文件最大 5TB</span>……也许我该尝试把超过 40 万的文件存放在不超过5TB的<span style=\"text-decoration: line-through;\">虚拟磁盘</span>文件里。(花了很多时间搜索和尝试了各种虚拟磁盘和 FUSE 文件系统软件，结果有的只能读取，有的不能即时写入，有的不能用<img src=\"/img/smiles/tv/16.gif\" smileid=\"55\" alt=\"(bgm39)\">。很多软件都依赖 Loop 循环设备，然而 Colab 的 Linux 内核功能过于精简，缺少 loop 和 nbd 模块，导致无法运行。最后找到能用的软件有 fuse2fs 。dd创建文件，格式化为ext4，fuse2fs 挂载文件。Raw生肉单文件和虚拟硬盘、备份压缩软件相比缺少按需扩大容量、自动分卷的功能。）其它还有文件夹嵌套最多 20 级，每日上传 750GB 流量的限制。<br>所以这就很纠结了啊<img src=\"/img/smiles/tv/15.gif\" smileid=\"54\" alt=\"(bgm38)\">，Google的“无限容量”云盘，文件数量有40万限制，但是大文件又没有随机写入或分块同步的办法。如果按每文件 1GiB 来分块，那么总容量为390.625TiB，但是每次做出数据修改时，最少的同步量就有1GiB。若每文件100MiB，则总容量约为38T。<br><br>数据校验还有一个办法是网盘分享出来，让其他人帮你检查文件是否完好，可是：<br>1.在没有密码或报酬的时候恐怕没人会浪费时间和带宽帮你做这件事。<br>2.数据量太大，很快会达到服务的组织流量阈值。如果是冷存储对象存储服务则流量费太贵。<br>3.版权问题，一般自己制作的数据不会白给别人，别人生产的数据也不想被这样传播。<br>对于以上问题不靠谱的解决方案：查找并购买你信任的云服务商的大带宽无限流量云主机，挂载网盘，输入密码解密，在Tor或i2p匿名网络发布IPFS存储节点。 ","time":"2021-2-4 21:19","title":"尝试备份100TiB以下数据的方案","userId":"magsom","userName":"magsom","userSign":"(Digital Lumpen Proletariat)"}