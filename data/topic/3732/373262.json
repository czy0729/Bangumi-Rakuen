{"id":373262,"avatar":"//lain.bgm.tv/pic/user/m/000/02/41/24149.jpg","floor":"#1","group":"～技术宅真可怕～","groupHref":"/group/a","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/11.jpg","message":"功能是将一篇网络文章以及引用的所有资源（图片、公式、音频、视频等）完整地存档到本地。<br><br>下载：<a href=\"https://lab.henix.info/get-article.html\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://lab.henix.info/get-article.html</a><br><br>简单地说就是想做成“网络文章”领域的 youtube-dl 。<br><br>对每个网站都专门处理，所以只能支持特定的网站。目前已支持<span style=\"font-weight:bold;\">微信公众号</span>和<span style=\"font-weight:bold;\">知乎专栏</span>。<br><br>使用场景：有时候我看到一篇高质量文章，我希望把它完整地保存到本地，以避免这篇文章突然消失。<br><br>文章可能因为各种原因突然消失：作者自己删了、被举报等等。<br><br>---<br><br>目前保存格式为一个目录里放 html 和全部资源文件。这样搜索内容只需要用 Windows 自带的搜索或其他普通的文件搜索工具（如 grep）。<br><br>---<br><br>同类工具有浏览器的“打印到 pdf”功能、笔记软件的网页剪藏。<br><br>不过个人认为这个工具因为对每个网站都专门处理，所以可以做得很精细：<br><br>1. 图片和视频都下载最高质量、最大尺寸<br>2. 修正外链：很多网站在跳转外链时会先进入一个自己的页面，get-article 保存时可以去除这一步骤<br><br>---<br><br>欢迎到 github 交流和反馈喵: <a href=\"https://github.com/henix/get-article-release\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">https://github.com/henix/get-article-release</a> ","time":"2022-9-24 11:23","title":"做了个网络文章存档工具 get-article","userId":"henix","userName":"henix","userSign":""}