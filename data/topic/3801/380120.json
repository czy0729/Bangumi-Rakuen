{"id":380120,"avatar":"//lain.bgm.tv/pic/user/m/000/35/13/351390.jpg","floor":"#1","group":"～技术宅真可怕～","groupHref":"/group/a","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/11.jpg","message":"当前环境下，GPT-4已经有了很强的写代码/编程的能力。这种能力不是简单笨拙的模仿，而是在理解了编程语义的基础上进行开发的。<br><br>关于GPT-4的技术细节很复杂，我自己也没有搞得懂，但是我可以从DeepMind公司的AlphaGo与AlphaGo Zero的发展中窥探AI的迭代之路。<br><br>为什么说，AI拥有自我迭代的能力呢？<br>引用论文：<a href=\"https://sci-hub.se/10.1145/3206157.3206174\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\" class=\"l\">Holcomb S D, Porter W K, Ault S V, et al. Overview on deepmind and its alphago zero ai[C]//Proceedings of the 2018 international conference on big data and education. 2018: 67-71.</a><br><br>——————————————————————————————————————————————————————<br><br>在这篇论文里讲述了AlphaGo与AlphaGo ZERO的区别<br><br>对于AlphaGO，其本质的特色在于学习，它是一种以学习为导向构筑的围棋模型。<br>在学习了千万级别的人类的棋谱之后，AlphaGo在下棋的过程中可以模仿人类的技巧进行对弈。<br><br>而AlphaGo Zero却有很大不同，AlphaGo Zero的提升在于自我博弈，自主技能迭代。也就是说，对于AlphaGo Zero它不需要人类作为导师，也不需要学习大量的人类棋盘，它可以通过自我博弈优化算法，最后提升棋类实力。<br><br>——————————————————————————————————————————————————————<div class=\"quote\"><q>The largest changes from AlphaGo and AlphaGo Zero are self\u0002play, use of only black and white stones on the board as input, and use a single artificial neural network for both value and policy. Zero, while able to be used in a distributed manner, \"uses just a single machine in the Google Cloud with 4 TPUs” for the sake of simplicity. It is a small list of changes, but each item contained involved complex work that needed to be done.<br>AlphaGo was fed large quantities of human-played games to learn the game whereas Zero learned purely through reinforcement playing itself. The entails the ability to learn with as little human guidance as possible and a larger step towards a more generalized AI ability. Reliance on human data can also be a limiting factor since large and complete datasets are not always readily available. Additionally, with the aim of learning from AI as well as the AI learning from us, allowing the freedom of unguided learning can provide alternate and unpredictable patterns to be discovered where human data could have interfered. Use of only black and white stones as input also provides a more generalized vantage by being closer to the input a human takes in when playing the game.</q></div>翻译：<br>AlphaGo和AlphaGo Zero最大的变化在于自我博弈、仅使用黑白棋子作为输入以及使用单个人工神经网络来处理价值和策略。Zero可以分布式使用，但出于简单起见，“仅使用谷歌云中带有4个TPU的单个计算机”。虽然变化列表很短，但每个条目都涉及需要完成的复杂工作。<br>AlphaGo通过学习人类玩过的大量游戏来学习游戏，而Zero则是通过自我强化学习纯粹地学习。这意味着能够尽可能少地依赖人类指导，并更大程度地实现通用人工智能的目标。依赖人类数据也可能成为限制因素，因为大而全的数据集并不总是容易获得。此外，为了从AI中学习，同时让AI从我们中学习，允许自由的无指导学习可以发现与人类数据干扰不同的替代和不可预测的模式。仅使用黑白棋子作为输入还提供了更接近人类下棋时所采取的视角，从而提供了一种更通用的视角。<br><br>——————————————————————————————————————————————————————<div class=\"quote\"><q>With the goal of improving an AI that can already defeat Go champions, even a marginal gain would be a significant accomplishment. AlphaGo Zero did more than a marginal gain. In a match against the previous version that defeated the Lee Sedol, AlphaGo Zero defeated its predecessor 100 – 0. A solid win shows a clear increase in performance. </q></div>翻译：<br>以提高已经能够击败围棋冠军的人工智能为目标，即使是微小的增益也是一个重大的成就。AlphaGo Zero不仅取得了微小的增益。在与击败李世石的先前版本对决中，<span style=\"font-weight:bold;\">AlphaGo Zero以100比0击败了其前辈</span>。坚实的胜利显示出明显的性能提升。<br><br>自我迭代模型AlphaGo Zero相比于学习模型AlphaGo Lee拥有碾压性的优势，这种优势不光体现在结果上，也体现在训练时间上，AlphaGo Zero用远少于学习型模型的时间与算力获得了更好更强的结果！<br><br>AI的模型最开始也是依赖编程而获得的架构，AI有了自我博弈的方向，AI有了编程的能力。<br><br>那么，AI通过编写代码，拥有自我进化，拥有自我迭代能力的未来还会远吗？<br><br>备注：<br>AI写代码：Mastropaolo A, Pascarella L, Guglielmi E, et al. On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot[J]. arXiv preprint arXiv:2302.00438, 2023.<br><br>AlphaGo Zero与GPT的基础模型相差甚远，但都运用了transformer技术。 ","time":"2023-4-7 13:53","title":"AI的自我迭代与创新","userId":"lostpig","userName":"𝙇𝙤𝙨𝙩 𝙋𝙞𝙜 🎉","userSign":"(被遗忘的小猪会被遗忘吗？)"}