{"id":364941,"avatar":"//lain.bgm.tv/pic/user/m/000/07/75/77515.jpg","floor":"#1","group":"～技术宅真可怕～","groupHref":"/group/a","groupThumb":"//lain.bgm.tv/pic/icon/m/000/00/00/11.jpg","message":"我最近在上网课学 CNN，想写个 online learning 的“制服/Lo裙”分类器去练习，然后我就想到可以从专门的网站上实时抓带有 tag 的图片去作为训练数据集；<br><br>我用 python 的 request 写了一个爬虫，然后去抓bcy网站上面的 JK 以及 lolita 的 api feed，这个过程倒是挺顺利，能抓到一大堆条目的预览数据（标题，网址，预览图，作者，tag等）；<br>爬虫的第二部分是打开条目的链接，进去找到图片，这样我就能得到已经标好 tag 的数据集了...... 然而，即使我写了足够像个人的 user-agent 还加上了 time.sleep(1) 的抓取间隔，在连续打开过十几次条目网址之后，抓到的东西就会变成 error，大概类似于这样：<br><br><div class=\"codeHighlight\"><pre><html><br><title></title><br><body>error</body><br></html></pre></div><br>俺寻思这是遇上了防抓取机制，想问问各位大佬有没有什么方法可以绕过去这种问题？<br>每次抓到 error 的时候去用浏览器打开网址就能看到一个白板网页上一行 error，然后隔半小时再去开这个网址就能看到正常的内容，所以我推断是<span style=\"background-color:#555;color:#555;border:1px solid #555;\">我太快了</span>在一定时间内同 ip 地址 request 数量异常导致服务器拒绝给出页面；如果要解决这个问题，一般来说应该把间隔放到多大才行？除了时间以外，有没有什么其他的解决方案呢？<br><br>万分感谢各位大佬的帮助！<br>（另，如果组内有bcy大佬的话我就先提前谢罪orz 求大佬饶我狗命不要鲨我<img src=\"/img/smiles/tv/15.gif\" smileid=\"54\" alt=\"(bgm38)\"> ","time":"2021-9-11 23:07","title":"网络爬虫的防抓取问题","userId":"th3ta","userName":"th3ta \"Paradox\"","userSign":"(Rigidity and Uncertainty~☆)"}